{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See also\n",
    "# https://github.com/tejank10/Flux-baselines/blob/master/baselines/dqn/dqn.jl\n",
    "# https://github.com/JuliaML/Reinforce.jl/blob/master/examples/mountain_car.jl\n",
    "\n",
    "using Base: Matrix\n",
    "using DataStructures\n",
    "using Flux\n",
    "using Plots\n",
    "using Reinforce\n",
    "using Reinforce: MaxIter, IterFunction, LearningStrategy, state, action, maxsteps\n",
    "using Reinforce.MountainCarEnv: MountainCar, MountainCarState"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "state_array (generic function with 1 method)"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gr()\n",
    "\n",
    "# Environment setup\n",
    "env = MountainCar()\n",
    "Reinforce.maxsteps(env::MountainCar) = 500\n",
    "state_array(state::MountainCarState) = [state.position, state.velocity]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "episode! (generic function with 2 methods)"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MEMORY = CircularBuffer{Tuple}(10000)\n",
    "\n",
    "function episode!(env, π)\n",
    "  ep = Episode(env, π)\n",
    "  for (s, a, r, sp) in ep\n",
    "    gui(plot(env))\n",
    "    push!(MEMORY, (state_array(s), a, r, state_array(sp), finished(env, sp)))\n",
    "  end\n",
    "  ep.total_reward, ep.niter\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deterministic policy that is solving the problem\n",
    "mutable struct BasicCarPolicy <: Reinforce.AbstractPolicy end\n",
    "Reinforce.action(policy::BasicCarPolicy, r, s, A) = s.velocity < 0 ? 1 : 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s = state(env) = MountainCarState(-1.0487946424807484, 0.021861897029926712)\n",
      "A = actions(env, s) = DiscreteSet{UnitRange{Int64}}(1:3)\n",
      "(STATE_SIZE, ACTION_SIZE) = (2, 3)\n",
      "model = Chain(Dense(STATE_SIZE, 10, relu), Dense(10, ACTION_SIZE)) = Chain(Dense(2, 10, NNlib.relu), Dense(10, 3))\n",
      "(model([1, 2])).data = Float32[-0.887088, -0.462151, -0.730609]\n"
     ]
    }
   ],
   "source": [
    "@show s = state(env)\n",
    "@show A = actions(env, s)\n",
    "\n",
    "STATE_SIZE = 2 # length(s)\n",
    "ACTION_SIZE = length(A)\n",
    "@show STATE_SIZE, ACTION_SIZE\n",
    "\n",
    "@show model = Chain(\n",
    "    Dense(STATE_SIZE, 10, relu),\n",
    "    Dense(10, ACTION_SIZE))\n",
    "loss(x, y) = Flux.mse(model(x), y)\n",
    "evalcb = () -> @show(loss(x, r))\n",
    "\n",
    "@show model([1, 2]).data\n",
    "\n",
    "function replay()\n",
    "    batch_size = length(MEMORY)\n",
    "    minibatch = sample(MEMORY, batch_size, replace = false)\n",
    "    \n",
    "    x = Matrix{Number}(undef, STATE_SIZE, batch_size)\n",
    "    y = Matrix{Number}(undef, ACTION_SIZE, batch_size)\n",
    "    \n",
    "    for (iter, (state, action, reward, next_state, done)) in enumerate(minibatch)\n",
    "        target = reward\n",
    "        if !done\n",
    "            target += 1.0 * maximum(model(next_state).data)\n",
    "        end\n",
    "\n",
    "        target_f = model(state).data\n",
    "        target_f[action] = target\n",
    "    \n",
    "        x[:, iter] .= state\n",
    "        y[:, iter] .= target_f\n",
    "    end\n",
    "     \n",
    "    Flux.train!(loss, Flux.params(model), [(x, y)], ADAM())\n",
    "end\n",
    "replay()\n",
    "\n",
    "mutable struct LearnedCarPolicy <: Reinforce.AbstractPolicy end\n",
    "Reinforce.maxsteps(env::MountainCar) = 500\n",
    "function Reinforce.action(π::LearnedCarPolicy, r, s, a)\n",
    "  act_values = model(state_array(s))\n",
    "  return Flux.onecold(act_values)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reward: -72.0, iter: 72\n"
     ]
    }
   ],
   "source": [
    "# Main part\n",
    "R, n = episode!(env, BasicCarPolicy())\n",
    "println(\"reward: $R, iter: $n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reward: -500.0, iter: 500\n"
     ]
    }
   ],
   "source": [
    "# Main part\n",
    "R, n = episode!(env, LearnedCarPolicy())\n",
    "println(\"reward: $R, iter: $n\")\n",
    "\n",
    "replay()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "ename": "InterruptException",
     "evalue": "InterruptException:",
     "output_type": "error",
     "traceback": [
      "InterruptException:",
      "",
      "Stacktrace:",
      " [1] #font#92(::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}, ::Function, ::String, ::Vararg{Any,N} where N) at /home/peterjdolan/.julia/packages/Plots/gylTh/src/components.jl:314",
      " [2] font(::String, ::Vararg{Any,N} where N) at /home/peterjdolan/.julia/packages/Plots/gylTh/src/components.jl:279",
      " [3] tickfont(::Plots.Axis) at /home/peterjdolan/.julia/packages/Plots/gylTh/src/utils.jl:1098",
      " [4] gr_display(::Plots.Subplot{Plots.GRBackend}, ::Measures.Length{:mm,Float64}, ::Measures.Length{:mm,Float64}, ::Array{Float64,1}) at /home/peterjdolan/.julia/packages/Plots/gylTh/src/backends/gr.jl:716",
      " [5] gr_display(::Plots.Plot{Plots.GRBackend}, ::String) at /home/peterjdolan/.julia/packages/Plots/gylTh/src/backends/gr.jl:535",
      " [6] gr_display at /home/peterjdolan/.julia/packages/Plots/gylTh/src/backends/gr.jl:493 [inlined]",
      " [7] _display(::Plots.Plot{Plots.GRBackend}) at /home/peterjdolan/.julia/packages/Plots/gylTh/src/backends/gr.jl:1395",
      " [8] display at /home/peterjdolan/.julia/packages/Plots/gylTh/src/output.jl:143 [inlined]",
      " [9] gui(::Plots.Plot{Plots.GRBackend}) at /home/peterjdolan/.julia/packages/Plots/gylTh/src/output.jl:132",
      " [10] episode!(::MountainCar, ::LearnedCarPolicy) at ./In[127]:6",
      " [11] top-level scope at ./In[224]:3"
     ]
    }
   ],
   "source": [
    "for i in 1:100\n",
    "    episode!(env, BasicCarPolicy())\n",
    "    episode!(env, LearnedCarPolicy())\n",
    "    replay()\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Params([Float32[-0.430641 -0.263542; -0.150519 0.185679; … ; -0.552938 -0.595616; 0.223367 0.375575] (tracked), Float32[0.002, -0.002, 0.002, 0.00199999, 0.002, 0.002, 0.002, -0.002, -0.002, 0.002] (tracked), Float32[0.310715 0.215176 … 0.116383 -0.445864; -0.399639 0.00915593 … 0.403386 -0.318224; -0.027919 0.623285 … -0.396398 -0.535551] (tracked), Float32[-0.002, -0.002, -0.002] (tracked)])"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Flux.params(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.0.2",
   "language": "julia",
   "name": "julia-1.0"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.0.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
